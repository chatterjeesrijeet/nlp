{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 0 : Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Lets first download the data set on which we will try the library functions. We will download the amazon food reviews dataset available at \"https://www.kaggle.com/snap/amazon-fine-food-reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset with 100000 records for the testing purpose overall it has more than 5.5 lakh reviews \n",
    "completeCorpusDF = pd.read_csv('./documents/reviews.csv',nrows = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeCorpusDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeCorpusDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now for us every Text entry is a document. So we can say that we have 1lakh documents, on which we will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Lets now write a function which will return me these documents in a list format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lisOfDocsFromDataFrame(df,feature = 'Text'):\n",
    "    \n",
    "    '''Returns a list of strs where each str is a food review'''\n",
    "    \n",
    "    #lets select the feature based on which you want to create the documents list \n",
    "    dataSeries = df[feature]\n",
    "    \n",
    "    # convert each document into string format and add using list comprehension \n",
    "    listOfStrDocs = [str(x) for x in dataSeries]\n",
    "    \n",
    "    #return the list of documents created to the calling function\n",
    "    return listOfStrDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfStrDocs = lisOfDocsFromDataFrame(completeCorpusDF,'Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Lets verify teh length, we should get the length as 1 lakh. and type must be list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listOfStrDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(listOfStrDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Lets print the first food review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfStrDocs[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Now we have two parts : \n",
    "     1. Clean the documents data\n",
    "     2. Create a tf-idf model or word-2-vec model or combination of them.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDocs(documents):\n",
    "    #create an empty container to hold the cleaned documents \n",
    "    smeCleanedDoc = []\n",
    "    #lets traverse the documents one by one and clean it \n",
    "    for d in documents:\n",
    "        # Remove Unicode\n",
    "        document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "        # Remove Mentions\n",
    "        document_test = re.sub(r'@\\w+', '', document_test)\n",
    "        # Lowercase the document\n",
    "        document_test = document_test.lower()\n",
    "        # Remove punctuations\n",
    "        document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "        # Lowercase the numbers\n",
    "        document_test = re.sub(r'[0-9]', '', document_test)\n",
    "        # Remove the doubled space\n",
    "        document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "        #append the cleaned doc to the container list\n",
    "        smeCleanedDoc.append(document_test)\n",
    "        \n",
    "    return smeCleanedDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemitization(Data):\n",
    "    \n",
    "    ''' lowers each word --> Removes stop words and ----> lemmatizes the documents !!!!'''\n",
    "    \n",
    "    #create the set of stop words \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #declaring container for the tokenised words for each documents \n",
    "    text_tokenised = []\n",
    "    \n",
    "    #creating the regex object for the punctuation keywords \n",
    "    punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "    \n",
    "    #traverse through the documents one by one \n",
    "    for text in Data:\n",
    "        #now remove all the punctuations from the text \n",
    "        text = punctuation.sub('', text)\n",
    "        #lower each words and then tokenize it \n",
    "        tokenized_words = word_tokenize(text.lower())\n",
    "        #now we process each tokenized words and then we keep it only if it is not in the stop words set\n",
    "        cleaned_tokenized_words = [w for w in tokenized_words if not w.lower() in stop_words]\n",
    "        #append each list of tokenized words to the main list of text_tokenised\n",
    "        text_tokenised.append(cleaned_tokenized_words)\n",
    "\n",
    "    #lets declare a container to keep the lemmatized words \n",
    "    text_lemitized = []\n",
    "    \n",
    "    #create a lemmatizer object \n",
    "    lemmatizer_wordnet = WordNetLemmatizer()\n",
    "    \n",
    "    # we will go through the each document text_list one by one present in text_tokenized\n",
    "    for text_list in text_tokenised:\n",
    "        #container for the lemmatized words \n",
    "        word_lemitized = []\n",
    "        # now for each word of the document(present as tokens in text_list) lets lemmatize\n",
    "        for text in text_list:\n",
    "            if text != '':\n",
    "                #add the lemmtized words to the document level container\n",
    "                word_lemitized.append(lemmatizer_wordnet.lemmatize(text, pos='v'))\n",
    "        #add processed documents one by one to the corpus level container\n",
    "        text_lemitized.append(word_lemitized)\n",
    "    \n",
    "    #now we want to put it back to the original form that is string \n",
    "    #create the corpus container \n",
    "    text_string = []\n",
    "    \n",
    "    #going through the documents one by one and then creating the string out of lemmatized tokens \n",
    "    for text in text_lemitized:\n",
    "        # creating string out of lemmatized tokens\n",
    "        text_string.append(' '.join(text))\n",
    "    #finally return the processed corpus \n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets cleans the corpus docs by calling cleanDocs function \n",
    "cleanedDFDocs = cleanDocs(listOfStrDocs)\n",
    "# cleanedDFDocs\n",
    "\n",
    "#for each document in the corpus lets tokenize , lemmatize remove stop words and save it back to the original format\n",
    "listOfDocs = token_lemitization(cleanedDFDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy several vitality can dog food products find good quality product look like stew process meat smell better labrador finicky appreciate product better'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfDocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As we can see this is a cleaned version of what we had initially ...\n",
    "    \n",
    "    'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy several vitality can dog food products find good quality product look like stew process meat smell better labrador finicky appreciate product better',\n",
       " 'product arrive label jumbo salt peanuts peanuts actually small size unsalted sure error vendor intend represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nut case filberts cut tiny square liberally coat powder sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduce edmund sell brother sisters witch',\n",
       " 'look secret ingredient robitussin believe find get addition root beer extract order good make cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfDocs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(listOfDocs))\n",
    "print(type(listOfDocs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1,1))\n",
    "# print(smeVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(listOfDocs)\n",
    "\n",
    "'''So basically X is now the list of lists, where each list is basically one doc that one review. And that \n",
    "   review is being represented as the tf-idf values for words in that review'''\n",
    "\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.21028982, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.13109558],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.27525774, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.25987195, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'addition', 'also', 'amaze', 'amount', 'appreciate', 'around', 'arrive', 'assortment', 'bag', 'beach', 'beer', 'believe', 'better', 'bite', 'black', 'brand', 'brother', 'buy', 'can', 'candy', 'case', 'cat', 'centuries', 'cherry', 'chewy', 'citrus', 'coat', 'complaint', 'confection', 'cut', 'deal', 'definitely', 'delightful', 'delivery', 'digestion', 'dog', 'eat', 'edmund', 'enjoyable', 'error', 'etc', 'every', 'everyone', 'expensive', 'extract', 'familiar', 'favorites', 'feed', 'filberts', 'find', 'finicky', 'five', 'flavor', 'flavorful', 'food', 'fralinger', 'gelatin', 'get', 'good', 'grape', 'grass', 'great', 'hair', 'happen', 'healthy', 'heaven', 'highly', 'husband', 'individually', 'ingredient', 'intend', 'jumbo', 'kid', 'label', 'labrador', 'last', 'lewis', 'liberally', 'licorice', 'light', 'like', 'lion', 'look', 'love', 'lover', 'make', 'many', 'meat', 'medicinal', 'melon', 'mostly', 'mouthful', 'much', 'none', 'nut', 'order', 'particular', 'party', 'peanuts', 'peppermint', 'piece', 'pillowy', 'pound', 'powder', 'price', 'process', 'product', 'products', 'puppies', 'quality', 'quick', 'recommend', 'red', 'represent', 'require', 'right', 'robitussin', 'root', 'rotate', 'rye', 'salt', 'saltwater', 'satisfy', 'secret', 'seduce', 'sell', 'serve', 'several', 'sisters', 'size', 'small', 'smell', 'soda', 'soft', 'sprout', 'square', 'stew', 'stick', 'story', 'sugar', 'sure', 'taffy', 'theme', 'tiny', 'together', 'treat', 'two', 'unsalted', 'vendor', 'version', 'vitality', 'wardrobe', 'watermelon', 'weeks', 'well', 'wheatgrass', 'wide', 'wild', 'witch', 'would', 'wrap', 'yummy']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.index('actually')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    It means every doc is now a list of size 163 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.20437499, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.40874999, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.17373742, 0.20437499,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.17373742, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.17373742, 0.20437499, 0.        , 0.        , 0.        ,\n",
       "       0.17373742, 0.        , 0.        , 0.        , 0.13513863,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.20437499, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.20437499, 0.        , 0.17373742, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.20437499, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.20437499, 0.34747483, 0.20437499, 0.        ,\n",
       "       0.20437499, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.20437499, 0.        ,\n",
       "       0.        , 0.        , 0.20437499, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.20437499, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.20437499, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suppose we check the first record .\n",
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first doc as a list of tokenised words : \n",
      "\n",
      " buy several vitality can dog food products find good quality product look like stew process meat smell better labrador finicky appreciate product better\n",
      "\n",
      "\n",
      "The corresponsing vectorized representation : \n",
      "\n",
      " [0.         0.         0.         0.         0.         0.20437499\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.40874999 0.         0.         0.         0.\n",
      " 0.17373742 0.20437499 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.17373742 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17373742 0.20437499 0.         0.\n",
      " 0.         0.17373742 0.         0.         0.         0.13513863\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.20437499 0.         0.\n",
      " 0.         0.         0.         0.20437499 0.         0.17373742\n",
      " 0.         0.         0.         0.         0.20437499 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.20437499 0.34747483\n",
      " 0.20437499 0.         0.20437499 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.20437499 0.         0.         0.\n",
      " 0.20437499 0.         0.         0.         0.         0.20437499\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.20437499 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "#against the first doc \n",
    "print(\"The first doc as a list of tokenised words : \\n\\n\",listOfDocs[0])\n",
    "print(\"\\n\\nThe corresponsing vectorized representation : \\n\\n\",X.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Second doc as a list of tokenised words : \n",
      "\n",
      " product arrive label jumbo salt peanuts peanuts actually small size unsalted sure error vendor intend represent product jumbo\n",
      "\n",
      "\n",
      "The corresponsing vectorized representation : \n",
      "\n",
      " [0.21028982 0.         0.         0.         0.         0.\n",
      " 0.         0.21028982 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21028982 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.21028982\n",
      " 0.42057964 0.         0.21028982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.42057964 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.35753112\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21028982 0.         0.         0.         0.         0.\n",
      " 0.         0.21028982 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21028982 0.17876556\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.21028982 0.         0.\n",
      " 0.         0.         0.         0.         0.21028982 0.21028982\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "#against the second doc \n",
    "print(\"The Second doc as a list of tokenised words : \\n\\n\",listOfDocs[1])\n",
    "print(\"\\n\\nThe corresponsing vectorized representation : \\n\\n\",X.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'addition',\n",
       " 'also',\n",
       " 'amaze',\n",
       " 'amount',\n",
       " 'appreciate',\n",
       " 'around',\n",
       " 'arrive',\n",
       " 'assortment',\n",
       " 'bag',\n",
       " 'beach',\n",
       " 'beer',\n",
       " 'believe',\n",
       " 'better',\n",
       " 'bite',\n",
       " 'black',\n",
       " 'brand',\n",
       " 'brother',\n",
       " 'buy',\n",
       " 'can',\n",
       " 'candy',\n",
       " 'case',\n",
       " 'cat',\n",
       " 'centuries',\n",
       " 'cherry',\n",
       " 'chewy',\n",
       " 'citrus',\n",
       " 'coat',\n",
       " 'complaint',\n",
       " 'confection',\n",
       " 'cut',\n",
       " 'deal',\n",
       " 'definitely',\n",
       " 'delightful',\n",
       " 'delivery',\n",
       " 'digestion',\n",
       " 'dog',\n",
       " 'eat',\n",
       " 'edmund',\n",
       " 'enjoyable',\n",
       " 'error',\n",
       " 'etc',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'expensive',\n",
       " 'extract',\n",
       " 'familiar',\n",
       " 'favorites',\n",
       " 'feed',\n",
       " 'filberts',\n",
       " 'find',\n",
       " 'finicky',\n",
       " 'five',\n",
       " 'flavor',\n",
       " 'flavorful',\n",
       " 'food',\n",
       " 'fralinger',\n",
       " 'gelatin',\n",
       " 'get',\n",
       " 'good',\n",
       " 'grape',\n",
       " 'grass',\n",
       " 'great',\n",
       " 'hair',\n",
       " 'happen',\n",
       " 'healthy',\n",
       " 'heaven',\n",
       " 'highly',\n",
       " 'husband',\n",
       " 'individually',\n",
       " 'ingredient',\n",
       " 'intend',\n",
       " 'jumbo',\n",
       " 'kid',\n",
       " 'label',\n",
       " 'labrador',\n",
       " 'last',\n",
       " 'lewis',\n",
       " 'liberally',\n",
       " 'licorice',\n",
       " 'light',\n",
       " 'like',\n",
       " 'lion',\n",
       " 'look',\n",
       " 'love',\n",
       " 'lover',\n",
       " 'make',\n",
       " 'many',\n",
       " 'meat',\n",
       " 'medicinal',\n",
       " 'melon',\n",
       " 'mostly',\n",
       " 'mouthful',\n",
       " 'much',\n",
       " 'none',\n",
       " 'nut',\n",
       " 'order',\n",
       " 'particular',\n",
       " 'party',\n",
       " 'peanuts',\n",
       " 'peppermint',\n",
       " 'piece',\n",
       " 'pillowy',\n",
       " 'pound',\n",
       " 'powder',\n",
       " 'price',\n",
       " 'process',\n",
       " 'product',\n",
       " 'products',\n",
       " 'puppies',\n",
       " 'quality',\n",
       " 'quick',\n",
       " 'recommend',\n",
       " 'red',\n",
       " 'represent',\n",
       " 'require',\n",
       " 'right',\n",
       " 'robitussin',\n",
       " 'root',\n",
       " 'rotate',\n",
       " 'rye',\n",
       " 'salt',\n",
       " 'saltwater',\n",
       " 'satisfy',\n",
       " 'secret',\n",
       " 'seduce',\n",
       " 'sell',\n",
       " 'serve',\n",
       " 'several',\n",
       " 'sisters',\n",
       " 'size',\n",
       " 'small',\n",
       " 'smell',\n",
       " 'soda',\n",
       " 'soft',\n",
       " 'sprout',\n",
       " 'square',\n",
       " 'stew',\n",
       " 'stick',\n",
       " 'story',\n",
       " 'sugar',\n",
       " 'sure',\n",
       " 'taffy',\n",
       " 'theme',\n",
       " 'tiny',\n",
       " 'together',\n",
       " 'treat',\n",
       " 'two',\n",
       " 'unsalted',\n",
       " 'vendor',\n",
       " 'version',\n",
       " 'vitality',\n",
       " 'wardrobe',\n",
       " 'watermelon',\n",
       " 'weeks',\n",
       " 'well',\n",
       " 'wheatgrass',\n",
       " 'wide',\n",
       " 'wild',\n",
       " 'witch',\n",
       " 'would',\n",
       " 'wrap',\n",
       " 'yummy']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.64768883, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.76190497, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try to regenerate \n",
    "vectorizer.transform(['I liked the food as it included meat']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['srijeet ']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.81030585, 0.        , 0.        ],\n",
       "       [0.4541945 , 0.        , 0.        , 0.        , 0.54044255,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        ],\n",
       "       [0.54194774, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.65330828, 0.65330828, 0.        , 0.        ,\n",
       "        0.58600719, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027],\n",
       "       [0.4541945 , 0.        , 0.        , 0.        , 0.54044255,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.64485945,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.757092  , 0.757092  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027],\n",
       "       [0.54194774, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(listOfDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank_bm25.BM25Okapi"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeCorpusDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched 50,000 records in 0.001 seconds \n",
      "\n",
      "This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
      "Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\n",
      "This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n"
     ]
    }
   ],
   "source": [
    "query = \"meat food\"\n",
    "\n",
    "tokenized_query = query.lower().split(\" \")\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "results = bm25.get_top_n(tokenized_query, listOfStrDocs, n = 3)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f'Searched 50,000 records in {round(t1-t0,3) } seconds \\n')\n",
    "for i in results:\n",
    "   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crlf\n",
      "[0.         0.         0.         0.         0.         0.81030585\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#first feature value across all the documents \n",
    "print(smeVectorizer.get_feature_names()[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crlf',\n",
       " 'cross',\n",
       " 'directory',\n",
       " 'encapsulation',\n",
       " 'frame',\n",
       " 'injection',\n",
       " 'input',\n",
       " 'insufficient',\n",
       " 'script',\n",
       " 'site',\n",
       " 'sql',\n",
       " 'traversal',\n",
       " 'validation',\n",
       " 'xfs']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame and set the vocabulary as the index\n",
    "smedf = pd.DataFrame(X, index = smeVectorizer.get_feature_names())\n",
    "smeVectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crlf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cross</th>\n",
       "      <td>0.454195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directory</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encapsulation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frame</th>\n",
       "      <td>0.541948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injection</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653308</td>\n",
       "      <td>0.653308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insufficient</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>script</th>\n",
       "      <td>0.454195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757092</td>\n",
       "      <td>0.757092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traversal</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xfs</th>\n",
       "      <td>0.541948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5  \\\n",
       "crlf           0.000000  0.000000  0.000000  0.000000  0.000000  0.810306   \n",
       "cross          0.454195  0.000000  0.000000  0.000000  0.540443  0.000000   \n",
       "directory      0.000000  0.000000  0.000000  0.707107  0.000000  0.000000   \n",
       "encapsulation  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "frame          0.541948  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "injection      0.000000  0.653308  0.653308  0.000000  0.000000  0.586007   \n",
       "input          0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "insufficient   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "script         0.454195  0.000000  0.000000  0.000000  0.540443  0.000000   \n",
       "site           0.000000  0.000000  0.000000  0.000000  0.644859  0.000000   \n",
       "sql            0.000000  0.757092  0.757092  0.000000  0.000000  0.000000   \n",
       "traversal      0.000000  0.000000  0.000000  0.707107  0.000000  0.000000   \n",
       "validation     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "xfs            0.541948  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                 6        7  \n",
       "crlf           0.0  0.00000  \n",
       "cross          0.0  0.00000  \n",
       "directory      0.0  0.00000  \n",
       "encapsulation  1.0  0.00000  \n",
       "frame          0.0  0.00000  \n",
       "injection      0.0  0.00000  \n",
       "input          0.0  0.57735  \n",
       "insufficient   0.0  0.57735  \n",
       "script         0.0  0.00000  \n",
       "site           0.0  0.00000  \n",
       "sql            0.0  0.00000  \n",
       "traversal      0.0  0.00000  \n",
       "validation     0.0  0.57735  \n",
       "xfs            0.0  0.00000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cross frame script xfs',\n",
       " 'sql injection',\n",
       " 'sql injection',\n",
       " 'directory traversal',\n",
       " 'cross site script',\n",
       " 'crlf injection',\n",
       " 'encapsulation',\n",
       " 'insufficient input validation']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smeProcessedListOfDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets try FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cross frame script xfs',\n",
       " 'sql injection',\n",
       " 'sql injection',\n",
       " 'directory traversal',\n",
       " 'cross site script',\n",
       " 'crlf injection',\n",
       " 'encapsulation',\n",
       " 'insufficient input validation']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smeProcessedListOfDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for data in smeProcessedListOfDocs:\n",
    "    templist = data.split()\n",
    "    for ele in templist:\n",
    "        tokens.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cross',\n",
       " 'frame',\n",
       " 'script',\n",
       " 'xfs',\n",
       " 'sql',\n",
       " 'injection',\n",
       " 'sql',\n",
       " 'injection',\n",
       " 'directory',\n",
       " 'traversal',\n",
       " 'cross',\n",
       " 'site',\n",
       " 'script',\n",
       " 'crlf',\n",
       " 'injection',\n",
       " 'encapsulation',\n",
       " 'insufficient',\n",
       " 'input',\n",
       " 'validation']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FastText(size = 5, window = 1, min_count=1)  # instantiate\n",
    "model2 = FastText(size = 5, window = 1, min_count=1)  # instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.build_vocab(sentences = smeProcessedListOfDocs)\n",
    "\n",
    "#or \n",
    "\n",
    "model2.build_vocab(sentences = tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.corpus_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Don't know why !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train(sentences = smeProcessedListOfDocs, \\\n",
    "             total_examples = len(smeProcessedListOfDocs), \\\n",
    "             epochs = 10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.train(sentences = tokens, total_examples = len(tokens), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00196213 -0.02264865  0.00647933  0.00873034 -0.01423903]\n"
     ]
    }
   ],
   "source": [
    "print(model1.wv['injection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0514102e-05  1.9208936e-02  1.3234768e-02 -2.8512876e-02\n",
      "  1.5703226e-02]\n"
     ]
    }
   ],
   "source": [
    "print(model2.wv['injection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     If you do it in a single step : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FastText(smeProcessedListOfDocs, \\\n",
    "                  size = 5, window = 1, \\\n",
    "                  min_count=1, iter = 10, sorted_vocab = 1)\n",
    "\n",
    "model2 = FastText(tokens, size = 5, window = 1, min_count = 1, iter = 10, sorted_vocab = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00196213 -0.02264865  0.00647933  0.00873034 -0.01423903]\n"
     ]
    }
   ],
   "source": [
    "print(model1.wv['injection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0514102e-05  1.9208936e-02  1.3234768e-02 -2.8512876e-02\n",
      "  1.5703226e-02]\n"
     ]
    }
   ],
   "source": [
    "print(model2.wv['injection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "print(model2.corpus_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21552292"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('tahts', 'thats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastTextVocab = model2.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "print(FastTextVocab.raw_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max_vocab_size',\n",
       " 'min_count',\n",
       " 'sample',\n",
       " 'sorted_vocab',\n",
       " 'null_word',\n",
       " 'cum_table',\n",
       " 'raw_vocab',\n",
       " 'max_final_vocab',\n",
       " 'ns_exponent',\n",
       " 'effective_min_count',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " '_scan_vocab',\n",
       " 'scan_vocab',\n",
       " 'sort_vocab',\n",
       " 'prepare_vocab',\n",
       " 'add_null_word',\n",
       " 'create_binary_tree',\n",
       " 'make_cum_table',\n",
       " 'load',\n",
       " '_load_specials',\n",
       " '_adapt_by_suffix',\n",
       " '_smart_save',\n",
       " '_save_specials',\n",
       " 'save',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastTextVocab.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        say we pass the list of str directectly and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(smeProcessedListOfDocs, size = 4, window = 2, min_count=1, iter=10, sorted_vocab = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01840575,  0.02851389, -0.0385243 , -0.033298  ], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['injection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Results are different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01797504,  0.00742574,  0.03411415, -0.0176611 ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['injection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01284553, -0.01689508,  0.05819905, -0.02598026], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['jection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03329188, -0.01619121,  0.04661208, -0.01210395], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['Rejection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "# from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "\u001b[K     || 12.0 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from en-core-web-sm==2.2.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.11.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (20.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.24.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.9)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (58.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.50.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy>=2.2.0->en-core-web-sm==2.2.0) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/srijeetchatterjee/Library/Python/3.8/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jinja2->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: No module named spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8a5aa70d40b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         return config.from_disk(\n\u001b[1;32m    547\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config.cfg from /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-614d6de0ab0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-215849878264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp(remove_stopwords(strip_punctuation(strip_non_alphanum(str(i).lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5dd605c1a9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my name is srijeet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp(remove_stopwords('my name is srijeet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
